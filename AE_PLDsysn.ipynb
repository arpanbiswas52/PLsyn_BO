{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZCHtLnq2z_W",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Automated Experiments of PLD synthesis through BO\n",
    "\n",
    "- Problem domain expert/ Experimental Set up - Sumner Haris\n",
    "- BO modeling -- Arpan Biswas\n",
    "\n",
    "#Problem Description\n",
    "\n",
    "Build a BO framework-\n",
    "\n",
    "- Here we have an experimental set up of $WSE_2$ monolayer flims grown by PLD.\n",
    "\n",
    "- We want to optimize the control parameters for best experimental results.\n",
    "\n",
    "- The goal is to do an automated experiment set up through BO model- where we sequnetially learn and adaptively sample towards region of best experimental outcome (synthesis).\n",
    "\n",
    "- In this notebook, we build the framework of BO to connect with the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Import GP and BoTorch functions\n",
    "import gpytorch as gpt\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel, PiecewisePolynomialKernel, PeriodicKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from gpytorch.models import ExactGP\n",
    "\n",
    "## For Ramam data fit \n",
    "from scipy.signal import medfilt\n",
    "from lmfit.models import LorentzianModel, PolynomialModel\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from torch.optim import SGD\n",
    "from torch.optim import Adam\n",
    "from scipy.stats import norm\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class SimpleCustomGP(ExactGP, GPyTorchModel):\n",
    "    _num_outputs = 1  # to inform GPyTorchModel API\n",
    "\n",
    "    def __init__(self, train_X, train_Y):\n",
    "        # squeeze output dim before passing train_Y to ExactGP\n",
    "        super().__init__(train_X, train_Y.squeeze(-1), GaussianLikelihood())\n",
    "        self.mean_module = ConstantMean()\n",
    "        #self.mean_module = LinearMean(train_X.shape[-1])\n",
    "        self.covar_module = ScaleKernel(\n",
    "            #base_kernel=MaternKernel(nu=0.5, ard_num_dims=train_X.shape[-1]),\n",
    "            #base_kernel=RBFKernel(ard_num_dims=train_X.shape[-1]),\n",
    "            base_kernel=PeriodicKernel(ard_num_dims=train_X.shape[-1])\n",
    "            #base_kernel = PiecewisePolynomialKernel(power = 2, ard_num_dims=train_X.shape[-1]),\n",
    "        )\n",
    "        self.to(train_X)  # make sure we're on the right device/dtype\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class PLDsyn_BO():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        #print(\"Enter number of randomly selected starting samples: Recommended between 5 to 10\")\n",
    "        #start_samples = float(input(\"Enter starting samples: \"))\n",
    "        #num_start = int(start_samples)\n",
    "        num_start = 10\n",
    "        print(\"Enter number of BO iterations:\")\n",
    "        BO_iter = float(input(\"Enter BO iterations: \"))\n",
    "        N = int(BO_iter)\n",
    "\n",
    "        #Define parameter space\n",
    "        p = torch.linspace(0, 500, 15) #Pressure (Torr)\n",
    "        t = torch.linspace(160, 800, 15) #Temperature (degC)\n",
    "        e1 = torch.linspace(5, 65, 15) #laser1 energy (mJ)\n",
    "        e2 = torch.linspace(0, 138, 15) #laser2 energy (mJ)\n",
    "\n",
    "        X = [p, t, e1, e2]\n",
    "\n",
    "\n",
    "        #Fixed parameters of experimental setup\n",
    "        flow_rate1 = 5\n",
    "        flow_rate2 = 0\n",
    "        rep_rate1 = 2\n",
    "        rep_rate2 = 2\n",
    "        aper1_sizeX = 8\n",
    "        aper1_sizeY = 8\n",
    "        aper2_sizeX = 20\n",
    "        aper2_sizeY = 15\n",
    "        target1_dist = 5\n",
    "        target2_dist = 10\n",
    "        LR_contrast = -0.34\n",
    "        aper1_area = aper1_sizeX*aper1_sizeY\n",
    "        aper2_area = aper2_sizeX*aper2_sizeY\n",
    "\n",
    "\n",
    "        fix_params = [flow_rate1, flow_rate2, rep_rate1, rep_rate2, aper1_sizeX, aper1_sizeY, aper1_area,\\\n",
    "                      aper2_sizeX, aper2_sizeY, aper2_area, target1_dist, target2_dist, LR_contrast]\n",
    "\n",
    "        print(\"#################################################################\")\n",
    "        print(\"Start Analysis\")\n",
    "        results_BO_opt = self.BO_PLD(X, fix_params, num_start, N)\n",
    "\n",
    "\n",
    "        print(\"#################################################################\")\n",
    "        np.save(\"optim_results\", results_BO_opt)\n",
    "        print(\"End of Analysis: Final results saved\")\n",
    "\n",
    "    #@title objective function evaluation- maximize peak2_prominance/peak2_FWHM\n",
    "    def lamb2shift(self, lamb,ex):\n",
    "        shift = 1e7/ex - 1e7/lamb\n",
    "        return shift\n",
    "\n",
    "    def SNV(self, I_data):\n",
    "        mean = I_data.mean()\n",
    "        std = I_data.std()\n",
    "\n",
    "        return (self, I_data-mean)/std\n",
    "\n",
    "    def MinMax(self, I_data):\n",
    "        mn = I_data.min()\n",
    "        mx = I_data.max()\n",
    "\n",
    "        return (I_data-mn)/(mx-mn)\n",
    "\n",
    "    def Despike_Norm(self, I_data):\n",
    "        despiked = medfilt(I_data, kernel_size=7)\n",
    "        return self.MinMax(despiked)\n",
    "\n",
    "    def get_WSe2_A1_FWHM_Si_ref_Raman(self, sample_file, reference_file,visualize=False):\n",
    "        excitation_wavelength = 532.11\n",
    "        # Get Raman spectrum from sample file, crop, despike, and normalize.\n",
    "        #ram_data = pd.read_csv(sample_file)\n",
    "        ram_data = sample_file\n",
    "        w0, I0 = ram_data['Wavelength'], ram_data['Intensity']\n",
    "        w, I = self.lamb2shift(w0, excitation_wavelength), I0\n",
    "        bounds = (w>=70)&(w<=700)\n",
    "        sub_x, sub_y = w[bounds], self.Despike_Norm(I[bounds])\n",
    "\n",
    "        #This is the cropped, despiked, and normalized Raman data\n",
    "        xdat, ydat = sub_x, sub_y\n",
    "\n",
    "        # Crop, despike, and normalize the reference Si spectrum\n",
    "        #ref_df = pd.read_csv(reference_file)\n",
    "        ref_df = reference_file\n",
    "        w0ref, I0ref = ref_df['Wavelength'], ref_df['Intensity']\n",
    "        w, I = self.lamb2shift(w0ref, excitation_wavelength), I0ref\n",
    "        sub_x, sub_y = w[bounds], I[bounds]\n",
    "\n",
    "        #This is the cropped, despiked and normalized Si substrate spectrum\n",
    "        subtrate_x, substrate_y = sub_x, self.Despike_Norm(sub_y)\n",
    "\n",
    "        # Substrate subtracted data\n",
    "        I_substrate_subtracted = ydat-substrate_y\n",
    "\n",
    "        # Create the Raman spectrum multi Lorentzian model\n",
    "        model = PolynomialModel(1, prefix='bkg_',nan_policy='omit')\n",
    "        params = model.make_params(c0=0,c1=0)\n",
    "\n",
    "        def add_peak(prefix, center, amplitude=1, sigma=10):\n",
    "            peak = LorentzianModel(prefix=prefix)\n",
    "            pars = peak.make_params()\n",
    "            pars[prefix + 'center'].set(center,min=center-10,max=center+10)\n",
    "            pars[prefix + 'amplitude'].set(amplitude,min=0)\n",
    "            pars[prefix + 'sigma'].set(sigma,min=2.5,max=40)\n",
    "            return peak, pars\n",
    "\n",
    "        rough_peak_positions = [117.4,230.1,247.3,364]#[LA2M_guess[i],351]\n",
    "        for j, cen in enumerate(rough_peak_positions):\n",
    "            peak, pars = add_peak('peak_%d_' % (j), cen)\n",
    "            model = model + peak\n",
    "            params.update(pars)\n",
    "\n",
    "        # Fit the model\n",
    "        init = model.eval(params, x=xdat)\n",
    "        result = model.fit(I_substrate_subtracted, params, x=xdat, max_nfev = 1000)\n",
    "        comps = result.eval_components()\n",
    "\n",
    "        if result.params['peak_2_fwhm'].stderr == None:\n",
    "            print('This is probably a bad fit, check and retry')\n",
    "\n",
    "        if visualize:\n",
    "            plt.plot(xdat, I_substrate_subtracted, label='data')\n",
    "            #plt.plot(xdat, result.best_fit, label='best fit')\n",
    "            for name, comp in comps.items():\n",
    "                if name != 'bkg_':\n",
    "                    plt.plot(xdat, comp+comps['bkg_'], '--', label=name)\n",
    "                    #print(np.max(comp+comps['bkg_']))\n",
    "                    #plt.plot(xdat, comp, '--', label=name)\n",
    "                else:\n",
    "                    plt.plot(xdat,comp,'--',label=name)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        return result, xdat\n",
    "\n",
    "    def func_obj(self, X, args):\n",
    "\n",
    "        # Sumner will connect to his experimental set up\n",
    "        # Pass params to experiments\n",
    "\n",
    "        # Experiments run backend and pass the output (Raman, PL etc)\n",
    "        gdown.download(\"https://drive.google.com/uc?id=1kKFTrto0KvQx3XjYH89rfMm_kLWAI541\", \"BO Suggestion 7_Ram.csv\", quiet=True)\n",
    "        gdown.download(\"https://drive.google.com/uc?id=1KQPqdVGLzg_TlYaN4Q-_oHqydPWZe76I\", \"BO Suggestion 7 Raman Ref 594C_Ram.csv\", quiet=True)\n",
    "        sample_file = pd.read_csv(\"BO Suggestion 7_Ram.csv\")\n",
    "        reference_file = pd.read_csv(\"BO Suggestion 7 Raman Ref 594C_Ram.csv\")\n",
    "\n",
    "        # To fit Statistical Ramam fit\n",
    "        result, xdat = self.get_WSe2_A1_FWHM_Si_ref_Raman(sample_file, reference_file, visualize=True)\n",
    "        fwhm = result.params['peak_2_fwhm'].value\n",
    "\n",
    "        comps = result.eval_components()\n",
    "        p_mat= comps['peak_2_']+comps['bkg_']\n",
    "        slope_p = np.zeros((len(p_mat)-1))\n",
    "        for i in range(0, len(p_mat)-1):\n",
    "            p1 = p_mat[i]\n",
    "            p2 = p_mat[i+1]\n",
    "            slope_p[i]= np.absolute(p1-p2)\n",
    "\n",
    "        pks, pks_ht = find_peaks(slope_p, height=1*max(slope_p))\n",
    "        peak_prom = peak_prominences(slope_p, pks)[0]\n",
    "\n",
    "        obj = (peak_prom*10e4)/fwhm  #To maximize\n",
    "        obj = torch.from_numpy(obj)*torch.randn(1)\n",
    "        return obj\n",
    "\n",
    "    def optimize_hyperparam_trainGP(self, train_X, train_Y):\n",
    "        # Gp model fit\n",
    "\n",
    "        gp_surro = SimpleCustomGP(train_X, train_Y)\n",
    "        gp_surro = gp_surro.double()\n",
    "        gp_surro.likelihood.noise_covar.register_constraint(\"raw_noise\", GreaterThan(1e-1))\n",
    "        mll1 = ExactMarginalLogLikelihood(gp_surro.likelihood, gp_surro)\n",
    "        # fit_gpytorch_model(mll)\n",
    "        mll1 = mll1.to(train_X)\n",
    "        gp_surro.train()\n",
    "        gp_surro.likelihood.train()\n",
    "        ## Here we use Adam optimizer with learning rate =0.1, user can change here with different algorithm and/or learning rate for each GP\n",
    "        optimizer1 = Adam([{'params': gp_surro.parameters()}], lr=0.1)\n",
    "        #optimizer1 = SGD([{'params': gp_surro.parameters()}], lr=0.0001)\n",
    "\n",
    "        NUM_EPOCHS = 150\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # clear gradients\n",
    "            optimizer1.zero_grad()\n",
    "            # forward pass through the model to obtain the output MultivariateNormal\n",
    "            output1 = gp_surro(train_X)\n",
    "            # Compute negative marginal log likelihood\n",
    "            loss1 = - mll1(output1, gp_surro.train_targets)\n",
    "            # back prop gradients\n",
    "            loss1.backward(retain_graph=True)\n",
    "            # print last iterations\n",
    "            if (epoch + 1) > NUM_EPOCHS: #Stopping the print for now\n",
    "                print(\"GP Model trained:\")\n",
    "                print(\"Iteration:\" + str(epoch + 1))\n",
    "                print(\"Loss:\" + str(loss1.item()))\n",
    "                # print(\"Length Scale:\" +str(gp_PZO.covar_module.base_kernel.lengthscale.item()))\n",
    "                print(\"noise:\" + str(gp_surro.likelihood.noise.item()))\n",
    "\n",
    "\n",
    "            optimizer1.step()\n",
    "\n",
    "        gp_surro.eval()\n",
    "        gp_surro.likelihood.eval()\n",
    "        return gp_surro\n",
    "\n",
    "    def cal_posterior(self, gp_surro, test_X, X, norm=False):\n",
    "\n",
    "        X1, X2, X3, X4 = X[0], X[1], X[2], X[3]\n",
    "        X1, X2, X3, X4 = torch.meshgrid(X1, X2, X3, X4)\n",
    "\n",
    "        if (norm==\"True\"):\n",
    "            X1 = (X1 - torch.min(X1))/(torch.max(X1)-torch.min(X1))\n",
    "            X2 = (X2 - torch.min(X2))/(torch.max(X2)-torch.min(X2))\n",
    "            X3 = (X3 - torch.min(X3))/(torch.max(X3)-torch.min(X3))\n",
    "            X4 = (X4 - torch.min(X4))/(torch.max(X4)-torch.min(X4))\n",
    "\n",
    "        #print(X1, X2, X3, X4)\n",
    "        y_pred_means_mat = torch.empty(len(test_X), 1) # Format for numerical cal\n",
    "        y_pred_means_mesh = torch.empty(X1.shape) #Format for visualization\n",
    "        y_pred_vars_mat = torch.empty(len(test_X), 1)\n",
    "        y_pred_vars_mesh = torch.empty(X1.shape)\n",
    "        t_X = torch.empty(1, test_X.shape[1])\n",
    "\n",
    "        with torch.no_grad(), gpt.settings.max_lanczos_quadrature_iterations(32), \\\n",
    "            gpt.settings.fast_computations(covar_root_decomposition=False, log_prob=False,\n",
    "                                                      solves=True), \\\n",
    "            gpt.settings.max_cg_iterations(100), \\\n",
    "            gpt.settings.max_preconditioner_size(80), \\\n",
    "            gpt.settings.num_trace_samples(128):\n",
    "            for t in range(0, len(test_X)):\n",
    "\n",
    "                for i in range(0, test_X.shape[1]):\n",
    "                    t_X[:, i] = test_X[t, i]\n",
    "                #t_X = test_X.double()\n",
    "                y_pred_surro = gp_surro.posterior(t_X)\n",
    "                y_pred_means_mat[t, 0] = y_pred_surro.mean\n",
    "                y_pred_vars_mat[t, 0] = y_pred_surro.variance\n",
    "\n",
    "\n",
    "            for t1, (x1, x2, x3, x4) in enumerate(zip(X1, X2, X3, X4)):\n",
    "                for t2, (xx1, xx2, xx3, xx4) in enumerate(zip(x1, x2, x3, x4)):\n",
    "                    for t3, (xxx1, xxx2, xxx3, xxx4) in enumerate(zip(xx1, xx2, xx3, xx4)):\n",
    "                        for t4, (xxxx1, xxxx2, xxxx3, xxxx4) in enumerate(zip(xxx1, xxx2, xxx3, xxx4)):\n",
    "\n",
    "                            t_X[:, 0] = xxxx1\n",
    "                            t_X[:, 1] = xxxx2\n",
    "                            t_X[:, 2] = xxxx3\n",
    "                            t_X[:, 3] = xxxx4\n",
    "                            #t_X = test_X.double()\n",
    "                            y_pred_surro = gp_surro.posterior(t_X)\n",
    "                            y_pred_means_mesh[t1, t2, t3, t4] = y_pred_surro.mean\n",
    "                            y_pred_vars_mesh[t1, t2, t3, t4] = y_pred_surro.variance\n",
    "\n",
    "        y_pred_means = [y_pred_means_mat, y_pred_means_mesh]\n",
    "        y_pred_vars = [y_pred_vars_mat, y_pred_vars_mesh]\n",
    "        return y_pred_means, y_pred_vars\n",
    "\n",
    "    def acqmanEI(self, y_means, y_vars, train_Y, ieval):\n",
    "\n",
    "\n",
    "        y_means = y_means.detach().numpy()\n",
    "        y_vars = y_vars.detach().numpy()\n",
    "        y_std = np.sqrt(y_vars)\n",
    "        fmax = train_Y.max()\n",
    "        fmax = fmax.detach().numpy()\n",
    "        best_value = fmax\n",
    "        EI_val = np.zeros(len(y_vars))\n",
    "        Z = np.zeros(len(y_vars))\n",
    "        eta = 0.01\n",
    "\n",
    "        for i in range(0, len(y_std)):\n",
    "            if (y_std[i] <=0):\n",
    "                EI_val[i] = 0\n",
    "            else:\n",
    "                Z[i] =  (y_means[i]-best_value-eta)/y_std[i]\n",
    "                EI_val[i] = (y_means[i]-best_value-eta)*norm.cdf(Z[i]) + y_std[i]*norm.pdf(Z[i])\n",
    "\n",
    "        # Eliminate evaluated samples from consideration to avoid repeatation in future sampling\n",
    "        EI_val[ieval] = -1\n",
    "        acq_val = np.max(EI_val)\n",
    "        acq_cand = [k for k, j in enumerate(EI_val) if j == acq_val]\n",
    "        #print(acq_val)\n",
    "        return acq_cand, acq_val, EI_val\n",
    "\n",
    "    def normalize_get_initialdata_PLD(self, X, fix_params, num, m):\n",
    "\n",
    "        X_feas = torch.empty((X[0].shape[0]*X[1].shape[0]*X[2].shape[0]*X[3].shape[0], 4))\n",
    "        k=0\n",
    "\n",
    "        for t1 in range(0, X[0].shape[0]):\n",
    "            for t2 in range(0, X[1].shape[0]):\n",
    "              for t3 in range(0, X[2].shape[0]):\n",
    "                for t4 in range(0, X[3].shape[0]):\n",
    "                    X_feas[k, 0] = X[0][t1]\n",
    "                    X_feas[k, 1] = X[1][t2]\n",
    "                    X_feas[k, 2] = X[2][t3]\n",
    "                    X_feas[k, 3] = X[3][t4]\n",
    "                    k=k+1\n",
    "\n",
    "        X_feas_norm = torch.empty((X_feas.shape[0], X_feas.shape[1]))\n",
    "        train_Y = torch.empty((num, 1))\n",
    "\n",
    "\n",
    "        # Normalize X\n",
    "        for i in range(0, X_feas.shape[1]):\n",
    "            X_feas_norm[:, i] = (X_feas[:, i] - torch.min(X_feas[:, i])) / (torch.max(X_feas[:, i]) - torch.min(X_feas[:, i]))\n",
    "\n",
    "\n",
    "        # Select starting samples randomly as training data\n",
    "        np.random.seed(0)\n",
    "        idx = np.random.randint(0, len(X_feas), num)\n",
    "        train_X = X_feas[idx]\n",
    "        train_X_norm = X_feas_norm[idx]\n",
    "\n",
    "        #Evaluate initial training data\n",
    "        x = torch.empty((1,train_X.shape[1]))\n",
    "\n",
    "        for i in range(0, num):\n",
    "            for j in range(0, train_X.shape[1]):\n",
    "              x[0, j] = train_X[i, j]\n",
    "            print(\"Function eval #\" + str(m + 1))\n",
    "\n",
    "            train_Y[i, 0] = self.func_obj(x, fix_params) # pass to the function where we will connect to experiments\n",
    "            m = m + 1\n",
    "\n",
    "        #print(train_Y)\n",
    "\n",
    "        return X_feas, X_feas_norm, train_X, train_X_norm, train_Y, idx, m\n",
    "\n",
    "    def augment_newdata_PLD(self, acq_X, acq_X_norm, train_X, train_X_norm, train_Y, fix_params, m):\n",
    "\n",
    "        nextX = acq_X\n",
    "        nextX_norm = acq_X_norm\n",
    "        #train_X_norm = torch.cat((train_X_norm, nextX_norm), 0)\n",
    "        #train_X_norm = train_X_norm.double()\n",
    "        train_X_norm = torch.vstack((train_X_norm, nextX_norm))\n",
    "        train_X = torch.vstack((train_X, nextX))\n",
    "\n",
    "        x = torch.empty((1,train_X.shape[1]))\n",
    "        for j in range(0, train_X.shape[1]):\n",
    "            x[0, j] = train_X[-1, j]\n",
    "\n",
    "\n",
    "        print(\"Function eval #\" + str(m + 1))\n",
    "        next_feval = torch.empty(1, 1)\n",
    "        next_feval[0, 0] = self.func_obj(x, fix_params) # pass to the function where we will connect to experiments\n",
    "        train_Y = torch.vstack((train_Y, next_feval))\n",
    "\n",
    "        m = m + 1\n",
    "        return train_X, train_X_norm, train_Y, m\n",
    "\n",
    "    def plot_iteration_results(self, train_X, train_X_norm, train_Y, test_X, test_X_norm, y_pred_means, y_pred_vars, fix_params, i):\n",
    "        # To minimization\n",
    "        y_pred_means_mat, y_pred_means_mesh = y_pred_means[0], y_pred_means[1]\n",
    "        y_pred_vars_mat, y_pred_vars_mesh = y_pred_vars[0], y_pred_vars[1]\n",
    "\n",
    "        #Best solution among the evaluated data\n",
    "\n",
    "        best_Y = torch.max(train_Y)\n",
    "        ind = torch.argmax(train_Y)\n",
    "        X_opt = torch.empty((1,train_X.shape[1]))\n",
    "        X_opt_norm = torch.empty((1,train_X.shape[1]))\n",
    "        for j in range(0, train_X.shape[1]):\n",
    "            X_opt[0, j] = train_X[ind, j]\n",
    "            X_opt_norm[0, j] = train_X_norm[ind, j]\n",
    "\n",
    "\n",
    "\n",
    "        # Best estimated solution from GP model considering the non-evaluated solution\n",
    "\n",
    "        best_estY = torch.max(y_pred_means_mat)\n",
    "        ind = torch.argmax(y_pred_means_mat)\n",
    "        X_opt_GP = torch.empty((1,test_X.shape[1]))\n",
    "        X_opt_GP_norm = torch.empty((1,test_X.shape[1]))\n",
    "        for j in range(0, test_X.shape[1]):\n",
    "            X_opt_GP[0, j] = test_X[ind, j]\n",
    "            X_opt_GP_norm[0, j] = test_X_norm[ind, j]\n",
    "\n",
    "\n",
    "        #Print results\n",
    "        print(\"#########################################################\")\n",
    "        print(\"Current best solutions at iteration:\" + str(i))\n",
    "        print(\"Best params among evaluated samples:\")\n",
    "        print(X_opt, torch.round(best_Y, decimals= 2))\n",
    "        print(\"Best params from GP map:\")\n",
    "        print(X_opt_GP, torch.round(best_estY, decimals= 2))\n",
    "\n",
    "        #Plot results\n",
    "        #Task to do\n",
    "        plt.figure()\n",
    "\n",
    "        fig,ax=plt.subplots(ncols=6, nrows= 2, figsize=(20,8))\n",
    "        k = 0\n",
    "\n",
    "        #P vs T\n",
    "        ymeans_x1x2 = torch.nanmean(torch.nanmean(y_pred_means_mesh, dim = 3), dim = 2)\n",
    "        yvars_x1x2 = torch.nanmean(torch.nanmean(y_pred_vars_mesh, dim = 3), dim = 2)\n",
    "        ymeans_x1x2 = torch.round(ymeans_x1x2, decimals= 2)\n",
    "        yvars_x1x2 = torch.round(yvars_x1x2, decimals =2)\n",
    "\n",
    "        #a = ax[0].imshow(y_pred_means_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        a = ax[0, 0].imshow(ymeans_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        ax[0, 0].scatter(train_X_norm[:,0], train_X_norm[:,1], c=train_Y, marker='o', cmap='Reds')\n",
    "        #ax[0, 0].scatter(X_opt[0, 0], X_opt[0, 1], marker='x', c='r')\n",
    "        #ax[0, 0].scatter(X_opt_GP[0, 0], X_opt_GP[0, 1], marker='o', c='r')\n",
    "        divider = make_axes_locatable(ax[0, 0])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(a, cax=cax, orientation='horizontal')\n",
    "        ax[0, 0].set_title('P vs T: Objective Mean')\n",
    "\n",
    "        #b = ax[1].imshow(y_pred_vars_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        b = ax[1, 0].imshow(yvars_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        divider = make_axes_locatable(ax[1, 0])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(b, cax=cax, orientation='horizontal')\n",
    "        ax[1, 0].set_title('P vs T: Objective Var')\n",
    "\n",
    "        #############################################\n",
    "\n",
    "        #P vs E1\n",
    "        ymeans_x1x2 = torch.nanmean(torch.nanmean(y_pred_means_mesh, dim = 3), dim = 1)\n",
    "        yvars_x1x2 = torch.nanmean(torch.nanmean(y_pred_vars_mesh, dim = 3), dim = 1)\n",
    "        ymeans_x1x2 = torch.round(ymeans_x1x2, decimals= 2)\n",
    "        yvars_x1x2 = torch.round(yvars_x1x2, decimals =2)\n",
    "        #a = ax[0].imshow(y_pred_means_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        a = ax[0, 1].imshow(ymeans_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        ax[0, 1].scatter(train_X_norm[:,0], train_X_norm[:,2], c=train_Y, marker='o', cmap='Reds')\n",
    "        #ax[0, 1].scatter(X_opt[0, 0], X_opt[0, 2], marker='x', c='r')\n",
    "        #ax[0, 1].scatter(X_opt_GP[0, 0], X_opt_GP[0, 2], marker='o', c='r')\n",
    "        divider = make_axes_locatable(ax[0, 1])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(a, cax=cax, orientation='horizontal')\n",
    "        ax[0, 1].set_title('P vs e1: Objective Mean')\n",
    "\n",
    "        #b = ax[1].imshow(y_pred_vars_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        b = ax[1, 1].imshow(yvars_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        divider = make_axes_locatable(ax[1, 1])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(b, cax=cax, orientation='horizontal')\n",
    "        ax[1, 1].set_title('P vs e1: Objective Var')\n",
    "\n",
    "        #############################################\n",
    "\n",
    "        #P vs E2\n",
    "        ymeans_x1x2 = torch.nanmean(torch.nanmean(y_pred_means_mesh, dim = 2), dim = 1)\n",
    "        yvars_x1x2 = torch.nanmean(torch.nanmean(y_pred_vars_mesh, dim = 2), dim = 1)\n",
    "        ymeans_x1x2 = torch.round(ymeans_x1x2, decimals= 2)\n",
    "        yvars_x1x2 = torch.round(yvars_x1x2, decimals =2)\n",
    "        #a = ax[0].imshow(y_pred_means_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        a = ax[0, 2].imshow(ymeans_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        ax[0, 2].scatter(train_X_norm[:,0], train_X_norm[:,3], c=train_Y, marker='o', cmap='Reds')\n",
    "        #ax[0, 2].scatter(X_opt[0, 0], X_opt[0, 3], marker='x', c='r')\n",
    "        #ax[0, 2].scatter(X_opt_GP[0, 0], X_opt_GP[0, 3], marker='o', c='r')\n",
    "        divider = make_axes_locatable(ax[0, 2])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(a, cax=cax, orientation='horizontal')\n",
    "        ax[0, 2].set_title('P vs e2: Objective Mean')\n",
    "\n",
    "        #b = ax[1].imshow(y_pred_vars_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        b = ax[1, 2].imshow(yvars_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        divider = make_axes_locatable(ax[1, 2])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(b, cax=cax, orientation='horizontal')\n",
    "        ax[1, 2].set_title('P vs e2: Objective Var')\n",
    "\n",
    "        #############################################\n",
    "\n",
    "        #T vs E1\n",
    "        ymeans_x1x2 = torch.nanmean(torch.nanmean(y_pred_means_mesh, dim = 3), dim = 0)\n",
    "        yvars_x1x2 = torch.nanmean(torch.nanmean(y_pred_vars_mesh, dim = 3), dim = 0)\n",
    "        ymeans_x1x2 = torch.round(ymeans_x1x2, decimals= 2)\n",
    "        yvars_x1x2 = torch.round(yvars_x1x2, decimals =2)\n",
    "        #a = ax[0].imshow(y_pred_means_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        a = ax[0, 3].imshow(ymeans_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        ax[0, 3].scatter(train_X_norm[:,1], train_X_norm[:,2], c=train_Y, marker='o', cmap='Reds')\n",
    "        #ax[0, 3].scatter(X_opt[0, 1], X_opt[0, 2], marker='x', c='r')\n",
    "        #ax[0, 3].scatter(X_opt_GP[0, 1], X_opt_GP[0, 2], marker='o', c='r')\n",
    "        divider = make_axes_locatable(ax[0, 3])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(a, cax=cax, orientation='horizontal')\n",
    "        ax[0, 3].set_title('T vs e1: Objective Mean')\n",
    "\n",
    "        #b = ax[1].imshow(y_pred_vars_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        b = ax[1, 3].imshow(yvars_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        divider = make_axes_locatable(ax[1, 3])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(b, cax=cax, orientation='horizontal')\n",
    "        ax[1, 3].set_title('T vs e1: Objective Var')\n",
    "\n",
    "        #############################################\n",
    "\n",
    "        #T vs E2\n",
    "        ymeans_x1x2 = torch.nanmean(torch.nanmean(y_pred_means_mesh, dim = 2), dim = 0)\n",
    "        yvars_x1x2 = torch.nanmean(torch.nanmean(y_pred_vars_mesh, dim = 2), dim = 0)\n",
    "        ymeans_x1x2 = torch.round(ymeans_x1x2, decimals= 2)\n",
    "        yvars_x1x2 = torch.round(yvars_x1x2, decimals =2)\n",
    "        #a = ax[0].imshow(y_pred_means_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        a = ax[0, 4].imshow(ymeans_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        ax[0, 4].scatter(train_X_norm[:,1], train_X_norm[:,3], c=train_Y, marker='o', cmap='Reds')\n",
    "        #ax[0, 4].scatter(X_opt[0, 1], X_opt[0, 3], marker='x', c='r')\n",
    "        #ax[0, 4].scatter(X_opt_GP[0, 1], X_opt_GP[0, 3], marker='o', c='r')\n",
    "        divider = make_axes_locatable(ax[0, 4])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(a, cax=cax, orientation='horizontal')\n",
    "        ax[0, 4].set_title('T vs e2: Objective Mean')\n",
    "\n",
    "        #b = ax[1].imshow(y_pred_vars_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        b = ax[1, 4].imshow(yvars_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        divider = make_axes_locatable(ax[1, 4])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(b, cax=cax, orientation='horizontal')\n",
    "        ax[1, 4].set_title('T vs e2: Objective Var')\n",
    "\n",
    "        #############################################\n",
    "\n",
    "        #E1 vs E2\n",
    "        ymeans_x1x2 = torch.nanmean(torch.nanmean(y_pred_means_mesh, dim = 1), dim = 0)\n",
    "        yvars_x1x2 = torch.nanmean(torch.nanmean(y_pred_vars_mesh, dim = 1), dim = 0)\n",
    "        ymeans_x1x2 = torch.round(ymeans_x1x2, decimals= 2)\n",
    "        yvars_x1x2 = torch.round(yvars_x1x2, decimals =2)\n",
    "        #a = ax[0].imshow(y_pred_means_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        a = ax[0, 5].imshow(ymeans_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        ax[0, 5].scatter(train_X_norm[:,2], train_X_norm[:,3], c=train_Y, marker='o', cmap='Reds')\n",
    "        #ax[0, 5].scatter(X_opt[0, 2], X_opt[0, 3], marker='x', c='r')\n",
    "        #ax[0, 5].scatter(X_opt_GP[0, 2], X_opt_GP[0, 3], marker='o', c='r')\n",
    "        divider = make_axes_locatable(ax[0, 5])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(a, cax=cax, orientation='horizontal')\n",
    "        ax[0, 5].set_title('e1 vs e2: Objective Mean')\n",
    "\n",
    "        #b = ax[1].imshow(y_pred_vars_mesh[:, :, 0, 0].detach().numpy(), origin='bottom', cmap='viridis')\n",
    "        b = ax[1, 5].imshow(yvars_x1x2.detach().numpy(), origin='lower', cmap='viridis', extent=(-0.1, 1.1, -0.1, 1.1))\n",
    "        divider = make_axes_locatable(ax[1, 5])\n",
    "        cax = divider.append_axes('bottom', size='5%', pad=0.25)\n",
    "        fig.colorbar(b, cax=cax, orientation='horizontal')\n",
    "        ax[1, 5].set_title('e1 vs e2: Objective Var')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        results_BO = [X_opt, best_Y, X_opt_GP, best_estY]\n",
    "        return results_BO\n",
    "\n",
    "    #@title BO framework- Integrating the above functions\n",
    "    def BO_PLD(self, X, fix_params, num_start, N):\n",
    "        num = num_start\n",
    "        m = 0\n",
    "        #best_eval = torch.empty(1)\n",
    "        #best_est = torch.empty(1)\n",
    "        # Initialization: evaluate few initial data normalize data\n",
    "        test_X, test_X_norm, train_X, train_X_norm, train_Y, idx, m = \\\n",
    "            self.normalize_get_initialdata_PLD(X, fix_params, num, m)\n",
    "\n",
    "\n",
    "        print(\"#################################################################\")\n",
    "        print(\"Initial evaluation complete. Start BO\")\n",
    "        print(\"#################################################################\")\n",
    "        ## Gp model fit\n",
    "        # Calling function to fit and optimizize Hyperparameter of Gaussian Process (using Adam optimizer)\n",
    "        # Input args- Torch arrays of normalized training data, parameter X and objective eval Y\n",
    "        # Output args- Gaussian process model lists\n",
    "        gp_surro = self.optimize_hyperparam_trainGP(train_X_norm, train_Y)\n",
    "\n",
    "        for i in range(1, N + 1):\n",
    "            # Calculate posterior for analysis for intermidiate iterations\n",
    "            y_pred_means, y_pred_vars = self.cal_posterior(gp_surro, test_X_norm, X, norm =\"True\")\n",
    "            y_pred_means_mat, y_pred_means_mesh = y_pred_means[0], y_pred_means[1]\n",
    "            y_pred_vars_mat, y_pred_vars_mesh = y_pred_vars[0], y_pred_vars[1]\n",
    "\n",
    "            if ((i == 1) or ((i % 3) == 0)):\n",
    "                # Plotting functions to check the current state exploration and Pareto fronts\n",
    "                results_BO = self.plot_iteration_results(train_X, train_X_norm, train_Y, test_X, test_X_norm, y_pred_means, y_pred_vars, fix_params, i)\n",
    "                if(i == 1):\n",
    "                    best_eval = results_BO[1]\n",
    "                    best_est = results_BO[3]\n",
    "                else:\n",
    "                    best_eval = torch.hstack((best_eval, results_BO[1]))\n",
    "                    best_est = torch.hstack((best_est, results_BO[3]))\n",
    "\n",
    "                fig, ax = plt.subplots(ncols=1, nrows= 1, figsize=(5,5))\n",
    "                ax.plot(best_eval, 'ro-', linewidth =2, label=\"best_eval\")\n",
    "                ax.plot(best_est, 'go-', linewidth =2, label=\"best_est\")\n",
    "                ax.legend(loc=\"best\")\n",
    "                #ax.set_xlabel(\"Iteration(x5)\")\n",
    "                ax.set_ylabel(\"function value\")\n",
    "                ax.set_title('Best function evaluations')\n",
    "                plt.show()\n",
    "\n",
    "            #print(idx)\n",
    "            acq_cand, acq_val, EI_val = self.acqmanEI(y_pred_means_mat, y_pred_vars_mat, train_Y, idx)\n",
    "            val = acq_val\n",
    "            ind = np.random.choice(acq_cand) # When multiple points have same acq values\n",
    "            idx = np.hstack((idx, ind))\n",
    "\n",
    "\n",
    "            ################################################################\n",
    "            ## Find next point which maximizes the learning through exploration-exploitation\n",
    "            if (i == 1):\n",
    "                val_ini = val\n",
    "            # Check for convergence\n",
    "            if ((val) == 0):  # Stop for negligible expected improvement\n",
    "                print(\"Model converged due to sufficient learning over search space \")\n",
    "                break\n",
    "            else:\n",
    "                nextX = torch.empty((1, len(X)))\n",
    "                nextX_norm = torch.empty(1, len(X))\n",
    "                nextX[0,:] = test_X[ind, :]\n",
    "                nextX_norm [0, :] = test_X_norm[ind, :]\n",
    "\n",
    "                # Evaluate true function for new data, augment data\n",
    "                train_X, train_X_norm, train_Y, m =\\\n",
    "                 self.augment_newdata_PLD(nextX, nextX_norm, train_X, train_X_norm, train_Y, fix_params, m)\n",
    "\n",
    "                # Gp model fit\n",
    "                # Updating GP with augmented training data\n",
    "                gp_surro = self.optimize_hyperparam_trainGP(train_X_norm, train_Y)\n",
    "\n",
    "        ## Final posterior prediction after all the sampling done\n",
    "\n",
    "        if (i == N):\n",
    "            print(\"#################################################################\")\n",
    "            print(\"Max. sampling reached, model stopped\")\n",
    "            print(\"#################################################################\")\n",
    "\n",
    "        #Optimal GP learning\n",
    "        gp_opt = gp_surro\n",
    "        # Posterior calculation with converged GP model\n",
    "        y_pred_means, y_pred_vars = self.cal_posterior(gp_opt, test_X_norm, X, norm =\"True\")\n",
    "\n",
    "        print(\"#################################################################\")\n",
    "        print(\"Display final solutions\")\n",
    "        # Plotting functions to check final iteration\n",
    "        results_BO_opt = self.plot_iteration_results(train_X, train_X_norm, train_Y, test_X, test_X_norm, y_pred_means, y_pred_vars, fix_params, i)\n",
    "\n",
    "        best_eval = torch.hstack((best_eval, results_BO[1]))\n",
    "        best_est = torch.hstack((best_est, results_BO[3]))\n",
    "        fig, ax = plt.subplots(ncols=1, nrows= 1, figsize=(5,5))\n",
    "        ax.plot(best_eval, 'ro-', linewidth =2, label=\"best_eval\")\n",
    "        ax.plot(best_est, 'go-', linewidth =2, label=\"best_est\")\n",
    "        ax.legend(loc=\"best\")\n",
    "        #ax.set_xlabel(\"Iteration(x5)\")\n",
    "        ax.set_ylabel(\"function value\")\n",
    "        ax.set_title('Best function evaluations')\n",
    "        plt.show()\n",
    "\n",
    "        return  results_BO_opt\n",
    "\n",
    "\n",
    "PLDsyn_BO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}